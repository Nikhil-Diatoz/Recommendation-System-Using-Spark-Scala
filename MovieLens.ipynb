{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.SparkContext\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Only one SparkContext should be running in this JVM (see SPARK-2243).The currently running SparkContext was created at:\r",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Only one SparkContext should be running in this JVM (see SPARK-2243).The currently running SparkContext was created at:\r",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r",
      "sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r",
      "java.lang.reflect.Constructor.newInstance(Unknown Source)\r",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r",
      "py4j.Gateway.invoke(Gateway.java:238)\r",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r",
      "py4j.GatewayConnection.run(GatewayConnection.java:238)\r",
      "java.lang.Thread.run(Unknown Source)\r",
      "  at org.apache.spark.SparkContext$.$anonfun$assertNoOtherContextIsRunning$2(SparkContext.scala:2531)\r",
      "  at scala.Option.foreach(Option.scala:407)\r",
      "  at org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2528)\r",
      "  at org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2618)\r",
      "  at org.apache.spark.SparkContext.<init>(SparkContext.scala:93)\r",
      "  at org.apache.spark.SparkContext.<init>(SparkContext.scala:153)\r",
      "  ... 37 elided\r",
      ""
     ]
    }
   ],
   "source": [
    "val sc = new SparkContext(master = \"local[*]\", appName = \"MovieLens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rawdata: org.apache.spark.rdd.RDD[String] = u.data MapPartitionsRDD[5] at textFile at <console>:26\r\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rawdata = sc.textFile(\"u.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: String = 196\t242\t3\t881250949\r\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// usernumber, recomended_movie_number, rating\n",
    "rawdata.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rawratings: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[7] at map at <console>:27\r\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rawratings = rawdata.map(_.split(\"\\t\").take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Creating an ML model starts from hear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.mllib.recommendation.ALS\r\n",
       "import org.apache.spark.mllib.recommendation.Rating\r\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.recommendation.ALS\n",
    "import org.apache.spark.mllib.recommendation.Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ratings: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[8] at map at <console>:29\r\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ratings = rawratings.map{ \n",
    "    case Array(user, movie, rating) => Rating(user.toInt, movie.toInt, rating.toDouble)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: org.apache.spark.mllib.recommendation.Rating = Rating(196,242,3.0)\r\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Training the data\n",
    "// parms:- rank=50, iteration=10, lambda=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.mllib.recommendation.MatrixFactorizationModel = org.apache.spark.mllib.recommendation.MatrixFactorizationModel@231f9357\r\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Train the model\n",
    "val model = ALS.train(ratings, 50, 10, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictedRating: Double = 2.3526242759635414\r\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Test the model\n",
    "val predictedRating = model.predict(789, 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating(789,76,5.434948836491207)\n",
      "Rating(789,39,5.378416728548247)\n",
      "Rating(789,447,5.317798470221051)\n",
      "Rating(789,502,5.14839473515516)\n",
      "Rating(789,185,5.1373519907926966)\n",
      "Rating(789,192,5.116849562116678)\n",
      "Rating(789,518,5.114898181453012)\n",
      "Rating(789,642,5.111137676354665)\n",
      "Rating(789,693,5.0819041932052995)\n",
      "Rating(789,127,5.012689437185902)\n",
      "Rating(789,475,4.98846703822459)\n",
      "Rating(789,276,4.988235698184952)\n",
      "Rating(789,741,4.9688601070427705)\n",
      "Rating(789,100,4.93640432974007)\n",
      "Rating(789,179,4.927150051029097)\n",
      "Rating(789,64,4.926808779889318)\n",
      "Rating(789,9,4.913925122488399)\n",
      "Rating(789,129,4.911489008309532)\n",
      "Rating(789,150,4.9078524892213)\n",
      "Rating(789,346,4.905457901596444)\n",
      "Rating(789,50,4.887630547990506)\n",
      "Rating(789,197,4.844415548068809)\n",
      "Rating(789,47,4.841324252988163)\n",
      "Rating(789,199,4.8043727121868915)\n",
      "Rating(789,56,4.7783510160726905)\n",
      "Rating(789,589,4.764352166391196)\n",
      "Rating(789,87,4.761068430383609)\n",
      "Rating(789,614,4.748048081344811)\n",
      "Rating(789,182,4.747400468684088)\n",
      "Rating(789,193,4.695709841575155)\n",
      "Rating(789,675,4.6625966891521236)\n",
      "Rating(789,12,4.654444091457706)\n",
      "Rating(789,214,4.638757741512798)\n",
      "Rating(789,172,4.626489063622286)\n",
      "Rating(789,1019,4.6224637813245115)\n",
      "Rating(789,11,4.62209379439714)\n",
      "Rating(789,144,4.596703567640615)\n",
      "Rating(789,154,4.593537266413101)\n",
      "Rating(789,921,4.592736131414835)\n",
      "Rating(789,187,4.537824845148641)\n",
      "Rating(789,425,4.5158543781457725)\n",
      "Rating(789,1226,4.509405343931577)\n",
      "Rating(789,639,4.490225378385091)\n",
      "Rating(789,23,4.458743267422106)\n",
      "Rating(789,4,4.4520241981020945)\n",
      "Rating(789,671,4.426964356226816)\n",
      "Rating(789,686,4.425354248302199)\n",
      "Rating(789,467,4.418614709532273)\n",
      "Rating(789,302,4.407876163962124)\n",
      "Rating(789,654,4.396285069355553)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "userId: Int = 789\r\n",
       "k: Int = 50\r\n",
       "topKRecs: Array[org.apache.spark.mllib.recommendation.Rating] = Array(Rating(789,76,5.434948836491207), Rating(789,39,5.378416728548247), Rating(789,447,5.317798470221051), Rating(789,502,5.14839473515516), Rating(789,185,5.1373519907926966), Rating(789,192,5.116849562116678), Rating(789,518,5.114898181453012), Rating(789,642,5.111137676354665), Rating(789,693,5.0819041932052995), Rating(789,127,5.012689437185902), Rating(789,475,4.98846703822459), Rating(789,276,4.988235698184952), Rating(789,741,4.9688601070427705), Rating(789,100,4.93640432974007), Rating(789,179,4.927150051029097), Rating(789,64,4.926808779889318), Rating(789,9,4.913925122488399), Rating(789,129,4.911489008309532), Rating(789,150,4.9078524892213), Rating(789,346,4.905457901596444), Ra...\r\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Generate recommendations\n",
    "val userId = 789\n",
    "val k = 50\n",
    "val topKRecs = model.recommendProducts(userId,k)\n",
    "println(topKRecs.mkString(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movies: org.apache.spark.rdd.RDD[String] = u.item MapPartitionsRDD[428] at textFile at <console>:28\r\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val movies = sc.textFile(\"u.item\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "titles: scala.collection.Map[Int,String] = Map(137 -> Big Night (1996), 891 -> Bent (1997), 550 -> Die Hard: With a Vengeance (1995), 1205 -> Secret Agent, The (1996), 146 -> Unhook the Stars (1996), 864 -> My Fellow Americans (1996), 559 -> Interview with the Vampire (1994), 218 -> Cape Fear (1991), 568 -> Speed (1994), 227 -> Star Trek VI: The Undiscovered Country (1991), 765 -> Boomerang (1992), 1115 -> Twelfth Night (1996), 774 -> Prophecy, The (1995), 433 -> Heathers (1989), 92 -> True Romance (1993), 1528 -> Nowhere (1997), 846 -> To Gillian on Her 37th Birthday (1996), 1187 -> Switchblade Sisters (1975), 1501 -> Prisoner of the Mountains (Kavkazsky Plennik) (1996), 442 -> Amityville Curse, The (1990), 1160 -> Love! Valour! Compassion! (1997), 101 -> Heavy Metal (1981), 1196 -> Sa...\r\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val titles = movies.map(line => line.split(\"\\\\|\").take(2)).map(x => (x(0).toInt, x(1))).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res12: String = Big Night (1996)\r\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles(137 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "moviesForUser: Seq[org.apache.spark.mllib.recommendation.Rating] = WrappedArray(Rating(789,1012,4.0), Rating(789,127,5.0), Rating(789,475,5.0), Rating(789,93,4.0), Rating(789,1161,3.0), Rating(789,286,1.0), Rating(789,293,4.0), Rating(789,9,5.0), Rating(789,50,5.0), Rating(789,294,3.0), Rating(789,181,4.0), Rating(789,1,3.0), Rating(789,1008,4.0), Rating(789,508,4.0), Rating(789,284,3.0), Rating(789,1017,3.0), Rating(789,137,2.0), Rating(789,111,3.0), Rating(789,742,3.0), Rating(789,248,3.0), Rating(789,249,3.0), Rating(789,1007,4.0), Rating(789,591,3.0), Rating(789,150,5.0), Rating(789,276,5.0), Rating(789,151,2.0), Rating(789,129,5.0), Rating(789,100,5.0), Rating(789,741,5.0), Rating(789,288,3.0), Rating(789,762,3.0), Rating(789,628,3.0), Rating(789,124,4.0))\r\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val moviesForUser = ratings.keyBy(_.user).lookup(789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "println(moviesForUser.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(English Patient, The (1996),1.0)\n",
      "(Big Night (1996),2.0)\n",
      "(Willy Wonka and the Chocolate Factory (1971),2.0)\n",
      "(Palookaville (1996),3.0)\n",
      "(Liar Liar (1997),3.0)\n",
      "(Toy Story (1995),3.0)\n",
      "(Tin Cup (1996),3.0)\n",
      "(Trees Lounge (1996),3.0)\n",
      "(Truth About Cats & Dogs, The (1996),3.0)\n",
      "(Ransom (1996),3.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "temp: Seq[(String, Double)] = ArrayBuffer((English Patient, The (1996),1.0), (Big Night (1996),2.0), (Willy Wonka and the Chocolate Factory (1971),2.0), (Palookaville (1996),3.0), (Liar Liar (1997),3.0), (Toy Story (1995),3.0), (Tin Cup (1996),3.0), (Trees Lounge (1996),3.0), (Truth About Cats & Dogs, The (1996),3.0), (Ransom (1996),3.0))\r\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val temp = moviesForUser.sortBy(_.rating).take(10).map(rating => (titles(rating.product), rating.rating))\n",
    "\n",
    "temp.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### medium Movie Lense recomendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.mllib.recommendation.ALS\r\n",
       "import org.apache.spark.mllib.recommendation.Rating\r\n",
       "import org.apache.spark.SparkConf\r\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Import other necessary packages\n",
    "import org.apache.spark.mllib.recommendation.ALS\n",
    "import org.apache.spark.mllib.recommendation.Rating\n",
    "import org.apache.spark.SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "18: error: ';' expected but 'for' found.\r",
     "output_type": "error",
     "traceback": [
      "<console>:18: error: ';' expected but 'for' found.\r",
      "       val predictedRating = {Predict for User 789 for movie 123}\r",
      "                                      ^\r",
      "<console>:21: error: illegal start of simple expression\r",
      "       val topKRecs = model.recommendProducts( *Recommend for User for the particular value of K* )\r",
      "                                                          ^\r",
      "<console>:34: error: ')' expected but '}' found.\r",
      "        }\r",
      "        ^\r",
      ""
     ]
    }
   ],
   "source": [
    "object Movie {\n",
    "    def main(args: Array[String]) {\n",
    "\n",
    "    val conf = new SparkConf().setAppName(\"Movie\").setMaster(\"local[2]\")\n",
    "    val sc = new SparkContext(conf)\n",
    "    val rawData = sc.textFile(\" *Read Data from Movie CSV file* \")\n",
    "\n",
    "    //rawData.first()\n",
    "    val rawRatings = rawData.map( *Split rawData on tab delimiter* )\n",
    "    val ratings = rawRatings.map { /*Map case array of User, Movie and Rating*/ }\n",
    "\n",
    "    //Training the data\n",
    "    val model = ALS.train(ratings, 50, 5, 0.01)\n",
    "    model.userFeatures\n",
    "    model.userFeatures.count\n",
    "    model.productFeatures.count\n",
    "    val predictedRating = {Predict for User 789 for movie 123}\n",
    "    val userId = *User 789*\n",
    "    val K = 10\n",
    "    val topKRecs = model.recommendProducts( *Recommend for User for the particular value of K* )\n",
    "    println(topKRecs.mkString(\"\\n\"))\n",
    "    val movies = sc.textFile(\" *Read Movie List Data* \")\n",
    "    val titles = movies.map(line => line.split(\"\\\\|\").take(2)).map(array => (array(0).toInt,array(1))).collectAsMap()\n",
    "    val titlesRDD = movies.map(line => line.split(\"\\\\|\").take(2)).map(array => (array(0).toInt,array(1))).cache()\n",
    "    titles(123)\n",
    "    val moviesForUser = ratings.*Search for User 789*\n",
    "    val sqlContext= *Create SQL Context*\n",
    "    val moviesRecommended = sqlContext.*Make a DataFrame of recommended movies*\n",
    "    moviesRecommended.registerTempTable(\"moviesRecommendedTable\")\n",
    "    sqlContext.sql(\"Select count(*) from moviesRecommendedTable\").foreach(println)\n",
    "    moviesForUser. *Sort the ratings for User 789* .map( *Map the rating to movie title* ). *Print the rating*\n",
    "    val results = moviesForUser.sortBy(-_.rating).take(30).map(rating => (titles(rating.product), rating.rating))\n",
    " }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
