{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://DESKTOP-8RE46GS.diatoz.com:4040\n",
       "SparkContext available as 'sc' (version = 3.0.1, master = local[*], app id = local-1612852754322)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation.RegressionEvaluator\r\n",
       "import org.apache.spark.ml.recommendation.ALS\r\n",
       "import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}\r\n",
       "import org.apache.spark.sql.SQLContext\r\n",
       "import org.apache.spark.sql.functions._\r\n",
       "import org.apache.spark.sql.types.DoubleType\r\n",
       "sqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@3fd884f3\r\n",
       "import sqlContext.implicits._\r\n",
       "import sys.process._\r\n",
       "import org.apache.spark.sql.SparkSession\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Importing necessary libraries\n",
    "\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator // to evaluvate our model\n",
    "import org.apache.spark.ml.recommendation.ALS //Alternating least square algorithm to create Recomendation model\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator} //Library to tune the Parameters\n",
    "import org.apache.spark.sql.SQLContext //enables to use Sql\n",
    "import org.apache.spark.sql.functions._ // imports usfull SQL functions\n",
    "import org.apache.spark.sql.types.DoubleType // a datatype which is used to store long Float values\n",
    "\n",
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc) //creating a spark context object\n",
    "\n",
    "import sqlContext.implicits._ // Implicit for converting Scala objects into DataFrames\n",
    "import sys.process._ //Enables to use shell commands with spark\n",
    "import org.apache.spark.sql.SparkSession //Importing SparkSession library\n",
    "\n",
    "import org.apache.spark.mllib.recommendation.Rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DatasetLink\n",
    "\n",
    "http://files.grouplens.org/datasets/movielens/ml-1m.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get-ChildItem : A positional parameter cannot be found that accepts argument 'to'.\r\n",
      "At line:1 char:1\r\n",
      "+ dir //shell command to see the list of files in the working directory\r\n",
      "+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n",
      "    + CategoryInfo          : InvalidArgument: (:) [Get-ChildItem], ParameterBindingExcep \r\n",
      "   tion\r\n",
      "    + FullyQualifiedErrorId : PositionalParameterNotFound,Microsoft.PowerShell.Commands.G \r\n",
      "   etChildItemCommand\r\n",
      " \r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!dir //shell command to see the list of files in the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ratings_rdd: org.apache.spark.rdd.RDD[String] = ./ml-1m/ratings.dat MapPartitionsRDD[1] at textFile at <console>:42\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ratings_rdd = sc.textFile(\"./ml-1m/ratings.dat\") //reading a text file using SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ratings = 1000209\n"
     ]
    }
   ],
   "source": [
    "println(\"Number of ratings = \" + ratings_rdd.count()) //Counts the number of rows present in the Rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1120::1513::4::974912892\n",
      "1317::66::1::975212462\n",
      "5820::953::5::957905663\n",
      "3804::2148::1::965969627\n",
      "5046::432::2::962507780\n",
      "5193::1230::4::961702574\n",
      "1635::1203::5::976942383\n",
      "1658::2124::3::974717583\n",
      "332::1633::5::982552828\n",
      "5074::3760::3::964468697\n"
     ]
    }
   ],
   "source": [
    "ratings_rdd.takeSample(false, 10, 0).foreach(println) // to view sample of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.mllib.recommendation.Rating\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Two options 1- to use built in module or creating an own class\n",
    "// case class Rating(userId: Int, movieId: Int, rating: Float) //notworking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ratings: org.apache.spark.sql.DataFrame = [user: int, product: int ... 1 more field]\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ratings = ratings_rdd.map(x => x.split(\"::\")).map(r => Rating(r(0).toInt, r(1).toInt, r(2).toDouble)).toDF().na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------+\n",
      "|user|product|rating|\n",
      "+----+-------+------+\n",
      "|  58|   2580|   4.0|\n",
      "| 136|    235|   5.0|\n",
      "| 187|    246|   5.0|\n",
      "| 329|    832|   2.0|\n",
      "| 368|   1370|   3.0|\n",
      "| 629|   1013|   4.0|\n",
      "| 660|   2409|   2.0|\n",
      "| 662|     19|   3.0|\n",
      "| 690|   1370|   5.0|\n",
      "| 852|    480|   3.0|\n",
      "+----+-------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Diffetrent ways to see the sample of data\n",
    "\n",
    "ratings.sample(false, 0.0001, seed=0).show(10)\n",
    "\n",
    "// ratings.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+\n",
      "|user|No.of ratings|\n",
      "+----+-------------+\n",
      "| 148|          624|\n",
      "| 463|          123|\n",
      "| 471|          105|\n",
      "| 496|          119|\n",
      "| 833|           21|\n",
      "|1088|         1176|\n",
      "|1238|           45|\n",
      "|1342|           92|\n",
      "|1580|           37|\n",
      "|1591|          314|\n",
      "+----+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "grouped_ratings: org.apache.spark.sql.DataFrame = [user: int, No.of ratings: bigint]\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Sample number of ratings per user\n",
    "val grouped_ratings = ratings.groupBy(\"user\").count().withColumnRenamed(\"count\", \"No.of ratings\")\n",
    "grouped_ratings.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users = 6040\n"
     ]
    }
   ],
   "source": [
    "// number of users in the dataset.\n",
    "println(\"Number of users = \" + grouped_ratings.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "#### Exploring Movies dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movies_df: org.apache.spark.rdd.RDD[String] = ./ml-1m/movies.dat MapPartitionsRDD[24] at textFile at <console>:44\r\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// importing dataset\n",
    "val movies_df = sc.textFile(\"./ml-1m/movies.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "699::To Cross the Rubicon (1991)::Drama\n",
      "854::Ballad of Narayama, The (Narayama Bushiko) (1958)::Drama\n",
      "3813::Interiors (1978)::Drama\n",
      "2505::8MM (1999)::Thriller\n",
      "3327::Beyond the Mat (2000)::Documentary\n",
      "3417::Crimson Pirate, The (1952)::Adventure|Comedy|Sci-Fi\n",
      "1062::Sunchaser, The (1996)::Drama\n",
      "1080::Monty Python's Life of Brian (1979)::Comedy\n",
      "197::Stars Fell on Henrietta, The (1995)::Drama\n",
      "3346::Color Me Blood Red (1965)::Horror\n"
     ]
    }
   ],
   "source": [
    "movies_df.takeSample(false,10,0).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "// // Convert movies data to a DataFrame\n",
    "\n",
    "// case class Movies(movieId: Int, Title: String, Genre: String)\n",
    "\n",
    "// val movie2 = movies_df.map(x => x.split(\"::\")).map(m =>new Movies(m(0).toInt, m(1).toString, m(2).toString )).toDF()\n",
    "// movie2.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movies: org.apache.spark.sql.DataFrame = [movieId: int, Title: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Convert movies data to a DataFrame\n",
    "\n",
    "val movies = movies_df.map { line =>\n",
    "      val fields = line.split(\"::\")\n",
    "      (fields(0).toInt, fields(1).toString, fields(2).toString)      // format: (movieId, movieName, catagory)\n",
    "    }.toDF(\"movieId\" ,  \"Title\", \"Genre\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|movieId|               Title|               Genre|\n",
      "+-------+--------------------+--------------------+\n",
      "|      1|    Toy Story (1995)|Animation|Childre...|\n",
      "|      2|      Jumanji (1995)|Adventure|Childre...|\n",
      "|      3|Grumpier Old Men ...|      Comedy|Romance|\n",
      "|      4|Waiting to Exhale...|        Comedy|Drama|\n",
      "|      5|Father of the Bri...|              Comedy|\n",
      "|      6|         Heat (1995)|Action|Crime|Thri...|\n",
      "|      7|      Sabrina (1995)|      Comedy|Romance|\n",
      "|      8| Tom and Huck (1995)|Adventure|Children's|\n",
      "|      9| Sudden Death (1995)|              Action|\n",
      "|     10|    GoldenEye (1995)|Action|Adventure|...|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// val movies_rdd = movies_rdd.withColumnRenamed(\"_1\", \"movieId\").withColumnRenamed(\"_2\", \"Title\").withColumnRenamed(\"_3\", \"Genre\")\n",
    "\n",
    "movies.show(10)\n",
    "// movies.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of movies = 3883\n"
     ]
    }
   ],
   "source": [
    "println(\"Number of movies = \" + movies.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Ratings data into Training (80%) and Test (20%) datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [user: int, product: int ... 1 more field]\r\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [user: int, product: int ... 1 more field]\r\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// we create two Arrays one for training and other for testing.\n",
    "val Array(training, test) = ratings.randomSplit(Array(0.8, 0.2), seed=0L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of ratings = 1000209\n",
      "Training dataset count = 799402, 79.92%\n",
      "Test dataset count = 200807, 20.08%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trainingRatio: Double = 79.92349598933824\r\n",
       "testRatio: Double = 20.076504010661772\r\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Show resulting Ratings dataset count\n",
    "var trainingRatio = training.count().toDouble/ratings.count().toDouble*100\n",
    "var testRatio = test.count().toDouble/ratings.count().toDouble*100\n",
    "\n",
    "println(\"Total number of ratings = \" + ratings.count())\n",
    "println(\"Training dataset count = \" + training.count() + \", \" + BigDecimal(trainingRatio).setScale(2, BigDecimal.RoundingMode.HALF_UP).toDouble + \"%\")\n",
    "println(\"Test dataset count = \" + test.count() + \", \" + BigDecimal(testRatio).setScale(2, BigDecimal.RoundingMode.HALF_UP).toDouble+ \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "// training.count().toDouble/ratings.count().toDouble //caleculating percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------+\n",
      "|user|product|rating|\n",
      "+----+-------+------+\n",
      "|  77|   2907|   1.0|\n",
      "| 161|    950|   5.0|\n",
      "| 224|   1569|   3.0|\n",
      "| 412|   3448|   4.0|\n",
      "| 466|   1073|   4.0|\n",
      "| 757|   2642|   2.0|\n",
      "| 796|   3068|   4.0|\n",
      "| 801|   1617|   3.0|\n",
      "| 841|   3916|   4.0|\n",
      "|1051|   2639|   2.0|\n",
      "+----+-------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.sample(false, 0.0001, seed=0).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "// BigDecimal(5.0) //creates 128 bit floating point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### ALS algorithm\n",
    "Parms of ALS algom\n",
    "- ratings: ,\n",
    "- rank: Int,\n",
    "- numUserBlocks: Int,\n",
    "- numItemBlocks: Int,\n",
    "- maxIter: Int,\n",
    "- regParam: Double,\n",
    "- implicitPrefs: Boolean,\n",
    "- alpha: Double,\n",
    "- nonnegative: Boolean,\n",
    "- intermediateRDDStorageLevel: ,\n",
    "- finalRDDStorageLevel: ,\n",
    "- checkpointInterval: Int,\n",
    "- seed: Long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "als: org.apache.spark.ml.recommendation.ALS = als_0c23a02c8180\r\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val als = new ALS().setMaxIter(10).setRegParam(0.1).setUserCol(\"user\").setItemCol(\"user\").setRatingCol(\"rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.recommendation.ALSModel = ALSModel: uid=als_0c23a02c8180, rank=10\r\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model = als.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "// println(als.explainParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions: org.apache.spark.sql.DataFrame = [user: int, product: int ... 2 more fields]\r\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions = model.transform(test).na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------+----------+\n",
      "|user|product|rating|prediction|\n",
      "+----+-------+------+----------+\n",
      "|1591|   3107|   5.0| 4.2886376|\n",
      "|1645|   3513|   1.0| 3.9887228|\n",
      "|1645|   3869|   5.0| 3.9887228|\n",
      "|2142|    589|   5.0|  3.485135|\n",
      "|3997|    429|   2.0|  3.474543|\n",
      "|5300|   1261|   4.0| 4.1530066|\n",
      "|5300|   1294|   4.0| 4.1530066|\n",
      "| 243|   1968|   3.0| 3.3926785|\n",
      "| 243|   2858|   5.0| 3.3926785|\n",
      "| 392|    110|   5.0| 3.4472303|\n",
      "+----+-------+------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.distinct().show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model using RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 1.0347301874121835\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "evaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = RegressionEvaluator: uid=regEval_41ecae40b541, metricName=rmse, throughOrigin=false\r\n",
       "rmse: Double = 1.0347301874121835\r\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluator = new RegressionEvaluator().setMetricName(\"rmse\").setLabelCol(\"rating\").setPredictionCol(\"prediction\")\n",
    "val rmse = evaluator.evaluate(predictions)\n",
    "println(\"Root-mean-square error = \" + rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "// evaluator.isLargerBetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\r\n",
       "Array({\r\n",
       "\tals_0c23a02c8180-regParam: 0.01\r\n",
       "}, {\r\n",
       "\tals_0c23a02c8180-regParam: 0.1\r\n",
       "})\r\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Building a Parameter Grid specifying what parameters and values will be evaluated in order to determine the best combination.\n",
    "val paramGrid = new ParamGridBuilder().addGrid(als.regParam, Array(0.01, 0.1)).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cv: org.apache.spark.ml.tuning.CrossValidator = cv_3ca9eb6a1284\r\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// cross validation with the defined parameter grid\n",
    "val cv = new CrossValidator().setEstimator(als).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\tals_0c23a02c8180-regParam: 0.01\n",
      "}\n",
      "{\n",
      "\tals_0c23a02c8180-regParam: 0.1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "// Validating the parameter grid values\n",
    "cv.getEstimatorParamMaps.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cvModel: org.apache.spark.ml.tuning.CrossValidatorModel = CrossValidatorModel: uid=cv_3ca9eb6a1284, bestModel=ALSModel: uid=als_0c23a02c8180, rank=10, numFolds=2\r\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Cross-evaluate to find the best model\n",
    "val cvModel = cv.fit(training) // cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best fit root-mean-square error = 1.032061455952174\n"
     ]
    }
   ],
   "source": [
    "println(\"Best fit root-mean-square error = \" + evaluator.evaluate(cvModel.transform(test).na.drop()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### recommend movies to a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userId: Int = 3000\r\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val userId = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------+\n",
      "|user|product|rating|\n",
      "+----+-------+------+\n",
      "|3000|   2987|   4.0|\n",
      "|3000|   2990|   3.0|\n",
      "|3000|   3793|   3.0|\n",
      "|3000|   1252|   4.0|\n",
      "|3000|   2997|   4.0|\n",
      "|3000|   1259|   3.0|\n",
      "|3000|    589|   4.0|\n",
      "|3000|      9|   1.0|\n",
      "|3000|   1265|   5.0|\n",
      "|3000|    733|   5.0|\n",
      "+----+-------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "movies_watched: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [user: int, product: int ... 1 more field]\r\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val movies_watched = ratings.filter(ratings(\"user\") === userId)\n",
    "movies_watched.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Future to_do List\n",
    "- Show user 3000's top 10 rated movies (with movie title, genre, and rating)\n",
    "- Determining what movies user 3000 has not already watched and rated so that we can make new movie recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res28: scala.collection.immutable.IndexedSeq[String] = Vector(f, o, o, 3)\r\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"foo3\".map(_.toString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "42: error: not found: value vegas\r",
     "output_type": "error",
     "traceback": [
      "<console>:42: error: not found: value vegas\r",
      "       import vegas._\r",
      "              ^\r",
      "<console>:43: error: not found: value vegas\r",
      "       import vegas.render.WindowRenderer._\r",
      "              ^\r",
      ""
     ]
    }
   ],
   "source": [
    "\n",
    "import vegas._\n",
    "import vegas.render.WindowRenderer._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "44: error: not found: value %\r",
     "output_type": "error",
     "traceback": [
      "<console>:44: error: not found: value %\r",
      "       %AddDeps org.vegas-viz vegas_2.11 {vegas-version} --transitive\r",
      "       ^\r",
      "<console>:44: error: object vegas is not a member of package org\r",
      "       %AddDeps org.vegas-viz vegas_2.11 {vegas-version} --transitive\r",
      "                    ^\r",
      "<console>:44: error: not found: value viz\r",
      "       %AddDeps org.vegas-viz vegas_2.11 {vegas-version} --transitive\r",
      "                          ^\r",
      "<console>:44: error: Double(0.11) does not take parameters\r",
      "       %AddDeps org.vegas-viz vegas_2.11 {vegas-version} --transitive\r",
      "                                         ^\r",
      "<console>:44: error: not found: value transitive\r",
      "       %AddDeps org.vegas-viz vegas_2.11 {vegas-version} --transitive\r",
      "                                                           ^\r",
      ""
     ]
    }
   ],
   "source": [
    "%AddDeps org.vegas-viz vegas_2.11 {vegas-version} --transitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "42: error: not found: value $ivy\r",
     "output_type": "error",
     "traceback": [
      "<console>:42: error: not found: value $ivy\r",
      "       import $ivy.`org.vegas-viz::vegas:0.3.0`\r",
      "              ^\r",
      ""
     ]
    }
   ],
   "source": [
    "import $ivy.`org.vegas-viz::vegas:0.3.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
