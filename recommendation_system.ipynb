{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation.RegressionEvaluator\r\n",
       "import org.apache.spark.ml.recommendation.ALS\r\n",
       "import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}\r\n",
       "import org.apache.spark.sql.SQLContext\r\n",
       "import org.apache.spark.sql.functions._\r\n",
       "import org.apache.spark.sql.types.DoubleType\r\n",
       "sqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@4f64a82b\r\n",
       "import sqlContext.implicits._\r\n",
       "import sys.process._\r\n",
       "import org.apache.spark.sql.SparkSession\r\n",
       "import org.apache.spark.mllib.recommendation.Rating\r\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Importing necessary libraries\n",
    "\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator // to evaluate our model\n",
    "import org.apache.spark.ml.recommendation.ALS //Alternating least square algorithm to create Recomendation model\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator} //Library to tune the Parameters\n",
    "import org.apache.spark.sql.SQLContext //Enables to use SparkSql\n",
    "import org.apache.spark.sql.functions._  // imports usfull SQL functions\n",
    "import org.apache.spark.sql.types.DoubleType // a datatype which is used to store long Float values\n",
    "\n",
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc) //creating a spark context object\n",
    "\n",
    "import sqlContext.implicits._ // Implicit for converting Scala objects into DataFrames\n",
    "import sys.process._  //Enables to use shell commands with spark\n",
    "import org.apache.spark.sql.SparkSession //Importing SparkSession library\n",
    "\n",
    "import org.apache.spark.mllib.recommendation.Rating //importing Rating module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DatasetLink\n",
    "\n",
    "http://files.grouplens.org/datasets/movielens/ml-1m.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "45: error: not found: value dir\r",
     "output_type": "error",
     "traceback": [
      "<console>:45: error: not found: value dir\r",
      "       !dir\r",
      "        ^\r",
      ""
     ]
    }
   ],
   "source": [
    "// To get the list of files present in the current working directory\n",
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ratings_rdd: org.apache.spark.rdd.RDD[String] = ./ml-1m/ratings.dat MapPartitionsRDD[1] at textFile at <console>:42\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ratings_rdd = sc.textFile(\"./ml-1m/ratings.dat\") //reading a text file using SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ratings = 1000209\n"
     ]
    }
   ],
   "source": [
    "println(\"Number of ratings = \" + ratings_rdd.count()) //Counts the number of rows present in the Rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1120::1513::4::974912892\n",
      "1317::66::1::975212462\n",
      "5820::953::5::957905663\n",
      "3804::2148::1::965969627\n",
      "5046::432::2::962507780\n",
      "5193::1230::4::961702574\n",
      "1635::1203::5::976942383\n",
      "1658::2124::3::974717583\n",
      "332::1633::5::982552828\n",
      "5074::3760::3::964468697\n"
     ]
    }
   ],
   "source": [
    "ratings_rdd.takeSample(false, 10, 0).foreach(println) // to view sample of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ratings: org.apache.spark.sql.DataFrame = [user: int, product: int ... 1 more field]\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Spliting the data bassed on \"::\" \n",
    "// converting the data types as needed  and conveting RDD to Dataframe\n",
    "val ratings = ratings_rdd.map(x => x.split(\"::\")).map(r => Rating(r(0).toInt, r(1).toInt, r(2).toDouble)).toDF().na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------+\n",
      "|user|product|rating|\n",
      "+----+-------+------+\n",
      "|  58|   2580|   4.0|\n",
      "| 136|    235|   5.0|\n",
      "| 187|    246|   5.0|\n",
      "| 329|    832|   2.0|\n",
      "| 368|   1370|   3.0|\n",
      "| 629|   1013|   4.0|\n",
      "| 660|   2409|   2.0|\n",
      "| 662|     19|   3.0|\n",
      "| 690|   1370|   5.0|\n",
      "| 852|    480|   3.0|\n",
      "+----+-------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.sample(false, 0.0001, seed=0).show(10)  //viewing the sample of structured data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+\n",
      "|user|No.of ratings|\n",
      "+----+-------------+\n",
      "| 148|          624|\n",
      "| 463|          123|\n",
      "| 471|          105|\n",
      "| 496|          119|\n",
      "| 833|           21|\n",
      "|1088|         1176|\n",
      "|1238|           45|\n",
      "|1342|           92|\n",
      "|1580|           37|\n",
      "|1591|          314|\n",
      "+----+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "grouped_ratings: org.apache.spark.sql.DataFrame = [user: int, No.of ratings: bigint]\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Sample number of ratings per user\n",
    "val grouped_ratings = ratings.groupBy(\"user\").count().withColumnRenamed(\"count\", \"No.of ratings\")\n",
    "grouped_ratings.show(10)  //viewing the result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users = 6040\n"
     ]
    }
   ],
   "source": [
    "// number of users in the dataset.\n",
    "println(\"Number of users = \" + grouped_ratings.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring Movies dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movies_df: org.apache.spark.rdd.RDD[String] = ./ml-1m/movies.dat MapPartitionsRDD[24] at textFile at <console>:44\r\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// importing movies dataset\n",
    "val movies_df = sc.textFile(\"./ml-1m/movies.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "699::To Cross the Rubicon (1991)::Drama\n",
      "854::Ballad of Narayama, The (Narayama Bushiko) (1958)::Drama\n",
      "3813::Interiors (1978)::Drama\n",
      "2505::8MM (1999)::Thriller\n",
      "3327::Beyond the Mat (2000)::Documentary\n",
      "3417::Crimson Pirate, The (1952)::Adventure|Comedy|Sci-Fi\n",
      "1062::Sunchaser, The (1996)::Drama\n",
      "1080::Monty Python's Life of Brian (1979)::Comedy\n",
      "197::Stars Fell on Henrietta, The (1995)::Drama\n",
      "3346::Color Me Blood Red (1965)::Horror\n"
     ]
    }
   ],
   "source": [
    "movies_df.takeSample(false,10,0).foreach(println) //viewing the sample of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movies: org.apache.spark.sql.DataFrame = [movieId: int, Title: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Preprocessing the movies data and Converting  it to a DataFrame \n",
    "val movies = movies_df.map { line =>\n",
    "      val fields = line.split(\"::\")\n",
    "      (fields(0).toInt, fields(1).toString, fields(2).toString)      // format: (movieId, movieName, catagory)\n",
    "    }.toDF(\"movieId\" ,  \"Title\", \"Genre\" ) // Renaming column names (movieId, Title, Genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|movieId|               Title|               Genre|\n",
      "+-------+--------------------+--------------------+\n",
      "|      1|    Toy Story (1995)|Animation|Childre...|\n",
      "|      2|      Jumanji (1995)|Adventure|Childre...|\n",
      "|      3|Grumpier Old Men ...|      Comedy|Romance|\n",
      "|      4|Waiting to Exhale...|        Comedy|Drama|\n",
      "|      5|Father of the Bri...|              Comedy|\n",
      "|      6|         Heat (1995)|Action|Crime|Thri...|\n",
      "|      7|      Sabrina (1995)|      Comedy|Romance|\n",
      "|      8| Tom and Huck (1995)|Adventure|Children's|\n",
      "|      9| Sudden Death (1995)|              Action|\n",
      "|     10|    GoldenEye (1995)|Action|Adventure|...|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies.show(10)   //viewing the result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of movies = 3883\n"
     ]
    }
   ],
   "source": [
    "println(\"Number of movies = \" + movies.count())  //Counts the number of rows present in the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spliting the Ratings data into Training (80%) and Test (20%) datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [user: int, product: int ... 1 more field]\r\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [user: int, product: int ... 1 more field]\r\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// we create two Arrays one for training and other for testing.\n",
    "val Array(training, test) = ratings.randomSplit(Array(0.8, 0.2), seed=0L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of ratings = 1000209\n",
      "Training dataset count = 799402, 79.92%\n",
      "Test dataset count = 200807, 20.08%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trainingRatio: Double = 79.92349598933824\r\n",
       "testRatio: Double = 20.076504010661772\r\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Show resulting Ratings dataset count\n",
    "//Calculating the percentile of  data added in training dataset wrt complete dataset\n",
    "var trainingRatio = training.count().toDouble/ratings.count().toDouble*100 \n",
    "\n",
    "//Calculating the percentile of  data added in testing dataset wrt complete dataset\n",
    "var testRatio = test.count().toDouble/ratings.count().toDouble*100\n",
    "\n",
    "println(\"Total number of ratings = \" + ratings.count())\n",
    "println(\"Training dataset count = \" + training.count() + \", \" + BigDecimal(trainingRatio).setScale(2, BigDecimal.RoundingMode.HALF_UP).toDouble + \"%\")\n",
    "println(\"Test dataset count = \" + test.count() + \", \" + BigDecimal(testRatio).setScale(2, BigDecimal.RoundingMode.HALF_UP).toDouble+ \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------+\n",
      "|user|product|rating|\n",
      "+----+-------+------+\n",
      "|  77|   2907|   1.0|\n",
      "| 161|    950|   5.0|\n",
      "| 224|   1569|   3.0|\n",
      "| 412|   3448|   4.0|\n",
      "| 466|   1073|   4.0|\n",
      "| 757|   2642|   2.0|\n",
      "| 796|   3068|   4.0|\n",
      "| 801|   1617|   3.0|\n",
      "| 841|   3916|   4.0|\n",
      "|1051|   2639|   2.0|\n",
      "+----+-------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.sample(false, 0.0001, seed=0).show(10)  //viewing the sample of result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### ALS algorithm used to create Recomendation model \n",
    "\n",
    "Parms of ALS algorithm\n",
    "- ratings: --RDD ,\n",
    "- rank: Int,\n",
    "- numUserBlocks: Int,\n",
    "- numItemBlocks: Int,\n",
    "- maxIter: Int,\n",
    "- regParam: Double,\n",
    "- implicitPrefs: Boolean,\n",
    "- alpha: Double,\n",
    "- nonnegative: Boolean,\n",
    "- intermediateRDDStorageLevel: ,\n",
    "- finalRDDStorageLevel: ,\n",
    "- checkpointInterval: Int,\n",
    "- seed: Long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "als: org.apache.spark.ml.recommendation.ALS = als_0c23a02c8180\r\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// creating an instance of ALS algorithm Library\n",
    "val als = new ALS().setMaxIter(10).setRegParam(0.1).setUserCol(\"user\").setItemCol(\"user\").setRatingCol(\"rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.recommendation.ALSModel = ALSModel: uid=als_0c23a02c8180, rank=10\r\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// training the Model\n",
    "val model = als.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "// println(als.explainParams) //built in function that gives the parmters used in AlS algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions: org.apache.spark.sql.DataFrame = [user: int, product: int ... 2 more fields]\r\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// transforming test dataset and droping the null values\n",
    "val predictions = model.transform(test).na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------+----------+\n",
      "|user|product|rating|prediction|\n",
      "+----+-------+------+----------+\n",
      "|1591|   3107|   5.0| 4.2886376|\n",
      "|1645|   3513|   1.0| 3.9887228|\n",
      "|1645|   3869|   5.0| 3.9887228|\n",
      "|2142|    589|   5.0|  3.485135|\n",
      "|3997|    429|   2.0|  3.474543|\n",
      "|5300|   1261|   4.0| 4.1530066|\n",
      "|5300|   1294|   4.0| 4.1530066|\n",
      "| 243|   1968|   3.0| 3.3926785|\n",
      "| 243|   2858|   5.0| 3.3926785|\n",
      "| 392|    110|   5.0| 3.4472303|\n",
      "+----+-------+------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.distinct().show(10) //getting the results of our model on Test Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model using RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 1.0347301874121835\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "evaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = RegressionEvaluator: uid=regEval_41ecae40b541, metricName=rmse, throughOrigin=false\r\n",
       "rmse: Double = 1.0347301874121835\r\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// creating an instance of  RegressionEvaluator with Rmse\n",
    "val evaluator = new RegressionEvaluator().setMetricName(\"rmse\").setLabelCol(\"rating\").setPredictionCol(\"prediction\")\n",
    "val rmse = evaluator.evaluate(predictions) //evaluating the model using predictions\n",
    "println(\"Root-mean-square error = \" + rmse) //printing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "// evaluator.isLargerBetter //checking is larger the Rmse value is better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\r\n",
       "Array({\r\n",
       "\tals_0c23a02c8180-regParam: 0.01\r\n",
       "}, {\r\n",
       "\tals_0c23a02c8180-regParam: 0.1\r\n",
       "})\r\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Building a Parameter Grid specifying what parameters and values will be evaluated in order to determine the best combination.\n",
    "val paramGrid = new ParamGridBuilder().addGrid(als.regParam, Array(0.01, 0.1)).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cv: org.apache.spark.ml.tuning.CrossValidator = cv_3ca9eb6a1284\r\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// cross validation with the defined parameter grid\n",
    "val cv = new CrossValidator().setEstimator(als).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\tals_0c23a02c8180-regParam: 0.01\n",
      "}\n",
      "{\n",
      "\tals_0c23a02c8180-regParam: 0.1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "// Validating the parameter grid values\n",
    "cv.getEstimatorParamMaps.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cvModel: org.apache.spark.ml.tuning.CrossValidatorModel = CrossValidatorModel: uid=cv_3ca9eb6a1284, bestModel=ALSModel: uid=als_0c23a02c8180, rank=10, numFolds=2\r\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Cross-evaluate to find the best model\n",
    "val cvModel = cv.fit(training) // cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best fit root-mean-square error = 1.032061455952174\n"
     ]
    }
   ],
   "source": [
    "println(\"Best fit root-mean-square error = \" + evaluator.evaluate(cvModel.transform(test).na.drop()))  //printing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking how our model Recommens movies to a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userId: Int = 3000\r\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val userId = 3000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------+\n",
      "|user|product|rating|\n",
      "+----+-------+------+\n",
      "|3000|   2987|   4.0|\n",
      "|3000|   2990|   3.0|\n",
      "|3000|   3793|   3.0|\n",
      "|3000|   1252|   4.0|\n",
      "|3000|   2997|   4.0|\n",
      "|3000|   1259|   3.0|\n",
      "|3000|    589|   4.0|\n",
      "|3000|      9|   1.0|\n",
      "|3000|   1265|   5.0|\n",
      "|3000|    733|   5.0|\n",
      "+----+-------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "movies_watched: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [user: int, product: int ... 1 more field]\r\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val movies_watched = ratings.filter(ratings(\"user\") === userId) //filtering based on userid\n",
    "movies_watched.show(10) //printing the results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
